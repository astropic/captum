{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from collections import defaultdict \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../../../../../../../notebooks/visual_genome/visual_genome_python_driver_master')\n",
    "sys.path.append('../../../../../../../../notebooks/visual_genome/visual_genome_python_driver_master/visual_genome')\n",
    "\n",
    "import visual_genome.local as vg\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all Scene Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"scene_graphs.npy\" has been pre-saved in a 800Mb npy file. It takes about 2h to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scene_graphs = np.load(\"scene_graphs.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Wordnet Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through Depth First Search:\n",
    "    \n",
    "1. Get the TOP parent synsets \"top_synset_list\" starting from synset target (e.g. \"zebra.n.01\") that are either:\n",
    "    - \"depth_up\" from target OR\n",
    "    - have no parent synset\n",
    "\n",
    "\n",
    "\n",
    "2. Get ALL synsets \"all_synset_list\" below the TOP parent synsets that are \"depth_down\" distant from each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recursive Depth First Search (DFS) auxiliar function\n",
    "def DFS_rec(synset, visited, depth, max_depth, top_synset_list, all_synset_list, up):\n",
    "\n",
    "    visited[synset] = True\n",
    "\n",
    "    all_synset_list.append(synset.name())\n",
    "    \n",
    "    depth += 1\n",
    "    if depth <= max_depth:\n",
    "        \n",
    "        if up:\n",
    "            next = synset.hypernyms()\n",
    "        else:\n",
    "            next = synset.hyponyms()\n",
    "        \n",
    "        if len(next) == 0:\n",
    "            top_synset_list.append(synset.name())\n",
    "            \n",
    "        for s in next:\n",
    "            if not visited[s]: \n",
    "                DFS_rec(s, visited, depth, max_depth, top_synset_list, all_synset_list, up=up)\n",
    "                \n",
    "    else:\n",
    "        top_synset_list.append(synset.name())\n",
    "\n",
    "# Depth First Search (DFS) entry function        \n",
    "def DFS(synset_name_list, max_depth, up=False):\n",
    "    \n",
    "    top_synset_list = []\n",
    "    all_synset_list = []\n",
    "\n",
    "    visited = defaultdict(bool)\n",
    "    for synset_name in synset_name_list:\n",
    "        DFS_rec(wn.synset(synset_name), visited, 0, max_depth, top_synset_list, all_synset_list, up=up)\n",
    "    \n",
    "    return top_synset_list, all_synset_list\n",
    "\n",
    "\n",
    "def get_synsets_from_target(target, depth_up, depth_down):\n",
    "    \n",
    "    max_depth = depth_up\n",
    "    top_synset_list, _ = DFS([target], max_depth, up=True)\n",
    "    \n",
    "    max_depth = depth_down\n",
    "    _ , all_synset_list = DFS(top_synset_list, max_depth)  \n",
    "    \n",
    "    return all_synset_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synset_list = get_synsets_from_target(\"car.n.01\", 1, 1)\n",
    "all_synset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Visual Genome Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the sysnsets that are in \"all_synset_list\", from Wordnet that are available in Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_object_synsets():\n",
    "\n",
    "    object_synsets = set()\n",
    "\n",
    "    for sg in scene_graphs:\n",
    "        for obj in sg.objects:\n",
    "            for syn in obj.synsets:\n",
    "                object_synsets.add(syn.name)\n",
    "                \n",
    "    return object_synsets\n",
    "\n",
    "all_object_synsets = get_all_object_synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_object_synsets = list(all_object_synsets & set(all_synset_list))\n",
    "target_object_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Concepts from Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the \"TOP_N\" most present concepts \"concept_set\" in the target (e.g. \"zebra\") when the Scene Graph (SG) contains:\n",
    "1. A relationship with \"OF\" or \"ON\" \"predicate\" and target is in the \"object\" OR\n",
    "2. A relationship with \"has\" \"predicate\" and target is in the \"subject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_concepts_from_target(target):\n",
    "    \n",
    "    concepts_set = defaultdict(int)\n",
    "    for sg in scene_graphs:\n",
    "        for rel in sg.relationships:\n",
    "        \n",
    "            if rel.predicate in [\"OF\", \"ON\"] and target in str(rel.object):\n",
    "                concepts_set[str(rel.subject).split()[-1]] += 1\n",
    "\n",
    "            elif rel.predicate in [\"has\"] and target in str(rel.subject):\n",
    "                concepts_set[str(rel.object).split()[-1]] += 1\n",
    "    \n",
    "    return concepts_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TOP_N = 5\n",
    "TARGET = \"car\"\n",
    "concepts_set = get_concepts_from_target(TARGET)\n",
    "concepts_set = list({k: v for k, v in sorted(concepts_set.items(), key=lambda item: item[1])})[-TOP_N:]\n",
    "concepts_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All Concepts Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all regions when the Scene Graph (SG) contains:\n",
    "\n",
    "1. A relationship with \"OF\" or \"ON\" \"predicate\" AND \n",
    "   synsets from \"all_synset_list\" are in the \"object\" AND\n",
    "   concepts from \"concepts_set\" are in the \"subject\" OR\n",
    "   \n",
    "\n",
    "2. A relationship with \"has\" \"predicate\" AND \n",
    "   synsets from \"all_synset_list\" are in the \"subject\" AND\n",
    "   concepts from \"concepts_set\" are in the \"object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_regionimages_from_concepts(concepts_set, synset_list):\n",
    "    \n",
    "    concept_regions = defaultdict(list)\n",
    "    \n",
    "    for concept in concepts_set:\n",
    "        for sg in scene_graphs:\n",
    "            for rel in sg.relationships:\n",
    "            \n",
    "                if rel.predicate in [\"OF\", \"ON\"] and concept in str(rel.subject):\n",
    "                    for synset in synset_list:\n",
    "                        if rel.object.synsets and synset == rel.object.synsets[0].name:\n",
    "                            concept_regions[concept].append((sg.image.id, rel.subject.x, rel.subject.y, rel.subject.width, rel.subject.height))\n",
    "                                \n",
    "                elif rel.predicate in [\"has\"] and concept in str(rel.object):\n",
    "                    for synset in synset_list:\n",
    "                        if rel.subject.synsets and synset == rel.subject.synsets[0].name:\n",
    "                            concept_regions[concept].append((sg.image.id, rel.object.x, rel.object.y, rel.object.width, rel.object.height))\n",
    "               \n",
    "    return concept_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concept_regions = get_regionimages_from_concepts(concepts_set, target_object_synsets)\n",
    "\n",
    "# Remove duplicates\n",
    "for concept in concept_regions:\n",
    "    concept_regions[concept] = list(set(concept_regions[concept]))\n",
    "\n",
    "print([(i, len(concept_regions[i])) for i in concept_regions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create N_CONCEPTS image concepts per concept name in designated disk folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def create_concept_images(concept_regions, margin=0):\n",
    "        \n",
    "    image_path = \"visual_genome/visual_genome_python_driver-master/visual_genome/data/images/\"\n",
    "    concepts_path = \"./concepts-vg/\"\n",
    "\n",
    "    for c, concept_name in enumerate(concept_regions):\n",
    "        \n",
    "        dir_path = os.path.join(concepts_path, concept_name)\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "        \n",
    "        #for i in range(len(concept_regions[concept_name])):\n",
    "        for i in range(N_CONCEPTS):\n",
    "        \n",
    "            image = concept_regions[concept_name][i]\n",
    "\n",
    "            img_id = image[0]\n",
    "            x = image[1]\n",
    "            y = image[2]\n",
    "            w = image[3]\n",
    "            h = image[4]\n",
    "\n",
    "            img_file = image_path + str(img_id) + '.jpg'\n",
    "            img = Image.open(img_file).crop((x, y, x+w, y+h)).convert(\"RGB\")\n",
    "            img = transform(img)\n",
    "                \n",
    "            save_path = os.path.join(dir_path, concept_name + '_{:04d}'.format(i) + '.jpg')\n",
    "            save_image(img, save_path)\n",
    "\n",
    "            \n",
    "N_CONCEPTS = 50\n",
    "create_concept_images(concept_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image as PIL_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_figs = 10\n",
    "n_concepts = len(concept_regions)\n",
    "image_path = 'visual_genome/visual_genome_python_driver-master/visual_genome/data/images/'\n",
    "margin = 0\n",
    "\n",
    "fig, axs = plt.subplots(n_concepts, n_figs + 1, figsize = (25, 4 * n_concepts))\n",
    "\n",
    "for c, concept_name in enumerate(concept_regions):\n",
    "\n",
    "    for i in range(n_figs + 1):\n",
    "            \n",
    "        if i == 0:\n",
    "            axs[c, i].text(1.0, 0.5, str(concept_name), ha='right', va='center', family='sans-serif', size=24)\n",
    "        else:\n",
    "            image = concept_regions[concept_name][i]\n",
    "            img_file = image_path + str(image[0]) + '.jpg'\n",
    "            img = mpimg.imread(img_file)\n",
    "            axs[c, i].add_patch(Rectangle((image[1], image[2]),\n",
    "                                image[3],\n",
    "                                image[4],\n",
    "                                fill=False,\n",
    "                                edgecolor='red',\n",
    "                                linewidth=3))\n",
    "            #img = img[image[2] - margin:image[2] + image[4] + margin, image[1] - margin:image[1] + image[3] + margin, :]\n",
    "            axs[c, i].imshow(img)\n",
    "            \n",
    "        axs[c, i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "723780601736333"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200727-000255",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "deps": [
     "//pytorch/captum:bento_kernel_deps"
    ],
    "external_deps": []
   },
   "no_uii": true,
   "notebook_number": "315802",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "288423125595128",
   "tags": "",
   "tasks": "",
   "title": "TCAV Graph - Visual Genome - Concepts Generation"
  },
  "kernelspec": {
   "display_name": "interpretability",
   "language": "python",
   "name": "bento_kernel_interpretability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
