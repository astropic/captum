{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob, sys\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import captum._utils.gradient as captum_grad\n",
    "\n",
    "sys.path.append('../../../../captum/concept/fb/')\n",
    "from _utils.common import concepts_to_str\n",
    "\n",
    "from captum.concept.fb._core.TCAV import TCAV\n",
    "from captum.concept.fb._core.concept import Concept\n",
    "from captum.concept.fb._utils.classifier import Classifier\n",
    "\n",
    "from captum.concept.fb._utils.data_iterator import get_concept_iterator_from_path, CustomIterableDataset\n",
    "\n",
    "import random\n",
    "from collections import defaultdict \n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Concept Activation Vectors (TCAV)\n",
    "\n",
    "This tutorial shows how to apply model interpretability using TCAVs.\n",
    "\n",
    "**Concepts** are high level features such as \"striped images\" in computer vision image classification tasks or the word \"female\" in a natural language processing classification task (even if the word female was not part of the dataset).\n",
    "\n",
    "With that in mind, using concepts as inputs in a pre-trained network, chosen intermediate layer activations are compared to baseline activations (say from random inputs) and, using a **linear classifier**, it is determined a \"degree of separation\" (the CAVs) between two (or more) concepts. These CAVs are used to compare then by a given metric (TCAV) to an attribution vector (the gradient of the network output wrt the layer output), for a test image.\n",
    "\n",
    "In this tutorial, we will demonstrate the use of TCAV in a Computer Vision classification task, using a pre-trained model on Imagenet.\n",
    "\n",
    "**Note:** Before running this tutorial, please install the torchvision, PIL, and matplotlib packages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Get images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get some images for testing. We are using for this tutorial the images from Imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Images - Imagenet\n",
    "imagenet_data = torchvision.datasets.ImageNet('./imagenet/',  split='val')\n",
    "SIZE = len(imagenet_data)\n",
    "\n",
    "# Method to normalize an image to Imagenet mean and standard deviation\n",
    "def transform(img):\n",
    "\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's define a few helper methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tensor_from_filename(filename):\n",
    "\n",
    "    img = Image.open(filename).convert(\"RGB\")\n",
    "    img = transform(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_imagenet_tensors(class_name):\n",
    "    \n",
    "    class_ind = imagenet_data.class_to_idx[class_name]\n",
    "    class_images = [imagenet_data[i][0] for i in range(SIZE) if imagenet_data.targets[i] == class_ind]\n",
    "    class_tensors = [transform(img) for img in class_images]\n",
    "    \n",
    "    return class_tensors, class_ind\n",
    "\n",
    "\n",
    "def save_image_tensors(tensor_list, root_path, class_name):\n",
    "    \n",
    "    for idx, tensor in enumerate(tensor_list):\n",
    "        path = os.path.join(root_path, class_name)\n",
    "        save_image(tensor, path + class_name + '_' + '{:04d}'.format(idx) + '.jpg')\n",
    "        \n",
    "\n",
    "def load_image_tensors(root_path, class_name):\n",
    "\n",
    "    path = os.path.join(root_path, class_name)\n",
    "    filenames = glob.glob(path + '*.jpg')\n",
    "    \n",
    "    tensors = []\n",
    "    for filename in filenames:\n",
    "        img = Image.open(filename).convert('RGB')\n",
    "        tensors.append(transform(img))\n",
    "    \n",
    "    return tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sample images from Imagenet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zebra_tensors, zebra_ind = get_imagenet_tensors('zebra')\n",
    "tiger_tensors, tiger_ind = get_imagenet_tensors('tiger')\n",
    "tench_tensors, tench_ind = get_imagenet_tensors('tench')\n",
    "bassinet_tensors, bassinet_ind = get_imagenet_tensors('bassinet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, save and/or load the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save tensor images\n",
    "\n",
    "# save_image_tensors(zebra_tensors, './images/', 'zebra')\n",
    "# save_image_tensors(tiger_tensors, './images/', 'tiger')\n",
    "# save_image_tensors(tench_tensors, './images/', 'tench')\n",
    "# save_image_tensors(bassinet_tensors, './images/', 'bassinet')\n",
    "\n",
    "\n",
    "# Load sample images from folder\n",
    "\n",
    "# zebra_tensors = load_image_tensors('./images/', 'zebra')\n",
    "# tiger_tensors = load_image_tensors('./images/', 'tiger')\n",
    "# tench_tensors = load_image_tensors('./images/', 'tench')\n",
    "# bassinet_tensors = load_image_tensors('./images/', 'bassinet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Get Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will use the Google LeNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.googlenet(pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Custom methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a few required custom methods that will be used as attribute functions for the TCAV library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a **CustomClassifier** class. It is required to define a class based on the *Classifier* abstract class. We have to define an **init** method (where we are choosing the *sklearn linear_model.SGDClassifier*, a **fit** method for the training, a **predict** method for the its training accuracy metric and a **weights** method, for the actual *interpret* TCAV method, i.e. the actual image test with a particular concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "class CustomClassifier(Classifier):\n",
    "    r\"\"\"\n",
    "        Linear Classifier class implementation for the Tests with\n",
    "        Concept Activation Vectors (TCAVs), as in the paper:\n",
    "            https://arxiv.org/pdf/1711.11279.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.lm = linear_model.SGDClassifier(alpha=0.01, max_iter=1000, tol=1e-3)\n",
    "\n",
    "    def fit(self, inputs, labels):\n",
    "\n",
    "        self.lm.fit(inputs.detach().numpy(), labels.detach().numpy())\n",
    "\n",
    "    def predict(self, inputs):\n",
    "\n",
    "        #print(torch.tensor(self.lm.predict(inputs.detach().numpy())))\n",
    "        return torch.tensor(self.lm.predict(inputs.detach().numpy()))\n",
    "\n",
    "    def weights(self):\n",
    "\n",
    "        if len(self.lm.coef_) == 1:\n",
    "            # if there are two concepts, there is only one label. We split it in two.\n",
    "            return torch.tensor([-1 * self.lm.coef_[0], self.lm.coef_[0]])\n",
    "        else:\n",
    "            return torch.tensor(self.lm.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to define a **layer_attributions** method. We also define here a helper method **get_module_from_name** to get the actual layer module whithin the model, from its concatenated hierarchical name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def get_module_from_name(model, name):\n",
    "#    return reduce(getattr, name.split(\".\"), model)\n",
    "\n",
    "\n",
    "def layer_attributions(model, layer_module, inputs, target, additional_forward_args):\n",
    "\n",
    "    #attrib = LayerIntergratedGradients().attribute\n",
    "    attrib, _ , _ = captum_grad.compute_layer_gradients_and_eval(\n",
    "                               model,\n",
    "                               layer_module,\n",
    "                               inputs, target_ind=target)\n",
    "\n",
    "    return attrib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define here another required method, **train_test_split** for splitting the *concepts* data into the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(x_list, y_list, test_split=0.33):\n",
    "\n",
    "    # Shuffle\n",
    "    z_list = list(zip(x_list, y_list))\n",
    "    random.shuffle(z_list)\n",
    "\n",
    "    # Split\n",
    "    test_size = int(test_split * len(z_list))\n",
    "    z_test, z_train = z_list[:test_size], z_list[test_size:]\n",
    "    x_test, y_test = zip(*z_test)\n",
    "    x_train, y_train = zip(*z_train)\n",
    "\n",
    "    x_train = torch.stack(x_train)\n",
    "    x_test = torch.stack(x_test)\n",
    "    y_train = torch.stack(y_train)\n",
    "    y_test = torch.stack(y_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4- Define Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now 2 helper methods for assembling concepts from folders and also generating random concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the simple **assemble_concepts** method reads the concepts from a list and, assuming these concept files are located in a folder with the same name, reads these files and creates a list of *Concept* objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assemble_concepts(concept_list, concepts_path=\"./concepts/\"):\n",
    "    \n",
    "    concepts = []\n",
    "    \n",
    "    for concept_name in concept_list:\n",
    "\n",
    "        concept_path = os.path.join(concepts_path, concept_name) + \"/\"\n",
    "        dataset = CustomIterableDataset(get_tensor_from_filename, concept_path)\n",
    "        concept_iter = get_concept_iterator_from_path(dataset)\n",
    "        concepts.append(Concept(id=len(concepts), name=concept_name, data_iter=concept_iter))\n",
    "    \n",
    "    return concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we create a method **gen_random_concepts** for generating random concepts. We create random concepts here using random images from Imagenet. These random concepts will be saved on a folder with the same name as its concept name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_random_concepts(imagenet_data, concept_name, concepts_path=\"./concepts/\", size=50):\n",
    "    \n",
    "    concept_path = os.path.join(concepts_path, concept_name) + \"/\"\n",
    "    \n",
    "    if not os.path.exists(concept_path):\n",
    "        os.mkdir(concept_path)\n",
    "    else:\n",
    "        files = glob.glob(concept_path + '*')\n",
    "        for f in files:\n",
    "            if os.path.isfile(f):\n",
    "                os.remove(f)\n",
    "    \n",
    "    # Random Tensors\n",
    "    random_images = [imagenet_data[np.random.randint(len(imagenet_data))][0] for i in range(size)]\n",
    "    random_tensors = [transform(img) for img in random_images]\n",
    "    \n",
    "    for idx, random_tensor in enumerate(random_tensors):\n",
    "        save_path = os.path.join(concept_path, 'random_' + '{:04d}'.format(idx) + '.jpg')\n",
    "        save_image(random_tensor, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run a vanilla example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Basic Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go step by step about the basics for the TCAV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate some random concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_random_concepts(imagenet_data, \"random1\", size=50)\n",
    "gen_random_concepts(imagenet_data, \"random2\", size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read and assemble the concepts from files into the Concepts class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concept_list = [\"striped\", \"random1\", \"random2\"]\n",
    "concepts_path=\"./concepts/\"\n",
    "\n",
    "concepts = assemble_concepts(concept_list, concepts_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some concepts samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_figs = 5\n",
    "n_concepts = len(concept_list)\n",
    "\n",
    "fig, axs = plt.subplots(n_concepts, n_figs + 1, figsize = (25, 4 * n_concepts))\n",
    "\n",
    "for c, concept_name in enumerate(concept_list):\n",
    "    \n",
    "    #plt.figtext(0.1, (n_concepts - c) / n_concepts, concept_name, ha=\"left\", va=\"top\", fontsize=21)\n",
    "    \n",
    "    concept_path = os.path.join(concepts_path, concept_name) + \"/\"\n",
    "    img_files = glob.glob(concept_path + '*')\n",
    "    \n",
    "    for i, img_file in enumerate(img_files[:n_figs + 1]):\n",
    "        if os.path.isfile(img_file):\n",
    "            \n",
    "            if i == 0:\n",
    "                axs[c, i].text(1.0, 0.5, str(concept_name), ha='right', va='center', family='sans-serif', size=24)\n",
    "            else:\n",
    "                img = mpimg.imread(img_file)\n",
    "                axs[c, i].imshow(img)\n",
    "\n",
    "            axs[c, i].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, lets create 2 experimental sets: **\\[\"striped\", \"random2\"\\]** and **\\[\"striped\", \"random2\"\\]**. They have to be assembled into Concept objects, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experimental_set_list = [[\"striped\", \"random1\"], [\"ceo\", \"random2\"]]\n",
    "experimental_set = []\n",
    "\n",
    "for concepts_list in experimental_set_list:\n",
    "    experimental_set.append(assemble_concepts(concepts_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the TCAV class, with the required attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mytcav = TCAV(model=model, \n",
    "              concepts=concepts,\n",
    "              layers=['inception4c', 'inception4d', 'inception4e'],\n",
    "              classifier=CustomClassifier(),\n",
    "              layer_attribution_func=layer_attributions,\n",
    "              train_test_split_func=train_test_split)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets actually \"TCAV\", i.e. Test with the computed CAVs. We will use the zebra images to try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = mytcav.interpret(inputs=torch.stack(zebra_tensors),\n",
    "                          experimental_set=experimental_set,\n",
    "                          target=zebra_ind)\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then present the results with a chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_float(f):\n",
    "    \n",
    "    if abs(f) >= 0.0005:\n",
    "        return '{:.3f}'.format(f)\n",
    "    else:\n",
    "        return '{:.3e}'.format(f)\n",
    "\n",
    "    \n",
    "layers=['inception4c', 'inception4d', 'inception4e']\n",
    "\n",
    "n_rows = 1\n",
    "n_figs = len(experimental_set_list)\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_figs, figsize = (25, 7 * n_rows))\n",
    "fs = 16\n",
    "\n",
    "barWidth = 1 / (len(experimental_set_list[0]) + 1)\n",
    "bars = np.zeros((len(experimental_set_list[0]), len(layers)))\n",
    "\n",
    "for idx_es, concepts in enumerate(experimental_set):\n",
    "\n",
    "    concepts = experimental_set[idx_es]\n",
    "    concepts_key = concepts_to_str(concepts)\n",
    "    for c, concept in enumerate(concepts):\n",
    "        for l, layer in enumerate(layers):\n",
    "            bars[c, l] = format_float(scores[concepts_key][layer]['magnitude'][c])\n",
    "\n",
    "    pos = []\n",
    "    pos.append(np.arange(len(layers)))\n",
    "    #pos.append([0.25, 1.25])\n",
    "    #print(pos)\n",
    "    for i in range(len(experimental_set_list[0]))[1:]:\n",
    "        pos.append([(x + barWidth) for x in pos[i-1]])\n",
    "        #print(pos)\n",
    "\n",
    "    for i in range(len(experimental_set_list[0])):\n",
    "        ax[idx_es].bar(pos[i], bars[i], width=barWidth, edgecolor='white', label=experimental_set_list[idx_es][i])\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    ax[idx_es].set_xlabel('Set 1', fontweight='bold', fontsize=fs)\n",
    "    ax[idx_es].set_xticks([r + 0.5 * barWidth for r in range(len(bars[0]))])\n",
    "    ax[idx_es].set_xticklabels(layers, fontsize=fs)\n",
    "\n",
    "    # Create legend & Show graphic\n",
    "    ax[idx_es].legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Advanced Run - Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, let's run many experiments. Dozens of them. And then graph statistics on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function to extract a population of results from a dictionary of Scores into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assemble_scores(scores, experimental_sets, idx, score_layer, score_type):\n",
    "    \n",
    "    score_list = []\n",
    "    for concepts in experimental_sets:\n",
    "        score_list.append(scores[\"-\".join([c for c in concepts])][score_layer][score_type][idx])\n",
    "        \n",
    "    return score_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a convenience function for assembling the experiments together as lists of Concept objects, creating and running the TCAV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_scores(model, layers, concept_list, experimental_set_list, inputs, target,\n",
    "               lm, layer_attributions, train_test_split, concepts_path=\"./concepts/\"):\n",
    "\n",
    "    # Invoke the function to convert a list of Concept names into a list of Concept objects.\n",
    "    concepts = assemble_concepts(concept_list, concepts_path)\n",
    "    \n",
    "    # Convert all experiment sets (concept name tuples) into a list of concept objects tuple.\n",
    "    experimental_set = []\n",
    "    for concepts_list in experimental_set_list:\n",
    "        experimental_set.append(assemble_concepts(concepts_list))\n",
    "\n",
    "    # Create a TCAV object\n",
    "    mytcav = TCAV(model, concepts, layers, lm, layer_attributions, train_test_split)\n",
    "\n",
    "    # Obtain a dictionary of scores for the desired inputs (test images)\n",
    "    scores = mytcav.interpret(inputs, experimental_set, target)\n",
    "        \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it is interesting to evaluate the p-values between the concept population pairs. We want to evaluate if these pupulations are overlapping (if both are random, for example) or if they are disjoint (if they are meant to be different, such as striped and random):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import ttest_1samp, ttest_ind\n",
    "\n",
    "def format_float(f):\n",
    "    \n",
    "    if abs(f) >= 0.0005 or abs(f) == 0.0:\n",
    "        return '{:.3f}'.format(f)\n",
    "    else:\n",
    "        return '{:.3e}'.format(f)\n",
    "\n",
    "def get_pval(scores, experimental_sets, score_layer, score_type, alpha=0.05, print_ret=False):\n",
    "    \n",
    "    P1 = assemble_scores(scores, experimental_sets, 0, score_layer, score_type)\n",
    "    P2 = assemble_scores(scores, experimental_sets, 1, score_layer, score_type)\n",
    "    \n",
    "    if print_ret:\n",
    "        print('P1[mean, std]: ', format_float(np.mean(P1)), format_float(np.std(P1)))\n",
    "        print('P2[mean, std]: ', format_float(np.mean(P2)), format_float(np.std(P2)))\n",
    "\n",
    "    tset, pval = ttest_ind(P1, P2)\n",
    "\n",
    "    if print_ret:\n",
    "        print(\"p-values:\", format_float(pval))\n",
    "\n",
    "    if pval < alpha:    # alpha value is 0.05 or 5%\n",
    "        relation = \"Disjoint\"\n",
    "        if print_ret:\n",
    "            print(\"Disjoint\")\n",
    "    else:\n",
    "        relation = \"Overlap\"\n",
    "        if print_ret:\n",
    "            print(\"Overlap\")\n",
    "        \n",
    "    return P1, P2, format_float(pval), relation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's assemble 3 important lists, containing the desired layers, concepts and the experiment concept pairs. In the end, we want to have defined:\n",
    "\n",
    "layer_list = **['inception4c']**\n",
    "\n",
    "concept_list = **['striped', 'ceo', 'random_0', 'random_1', 'random2', ... , 'random_n']**, n = n_exp (number of experiments)\n",
    "\n",
    "experimental_set = **[['striped', 'random_1'], ['striped', 'random_2'], ... , ['striped', 'random_n'],  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;['random_0', 'random_1'], ['random_0', 'random_2'], ... ,['random_0', 'random_n']]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = ['inception4c', 'inception4d']\n",
    "concept_list = ['striped', 'ceo']\n",
    "experimental_set_list = []\n",
    "\n",
    "concept_list_all = list(concept_list)\n",
    "\n",
    "n_exp = 5\n",
    "\n",
    "for i in range(n_exp + 1):\n",
    "    gen_random_concepts(imagenet_data, 'random_' + str(i), size=50)\n",
    "    concept_list_all.append('random_' + str(i))\n",
    "\n",
    "for c in concept_list:\n",
    "    for i in range(1, n_exp + 1):\n",
    "        experimental_set_list.append([c, 'random_' + str(i)])\n",
    "    \n",
    "for i in range(1, n_exp + 1):\n",
    "    experimental_set_list.append(['random_0', 'random_' + str(i)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the TCAV and obtain the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Run TCAV\n",
    "scores = get_scores(model, layer_list, concept_list_all, experimental_set_list,\n",
    "                    torch.stack(zebra_tensors), zebra_ind, CustomClassifier(), layer_attributions, \n",
    "                    train_test_split)\n",
    "\n",
    "scores\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel:      82s (-34%)\n",
    "# Non-parallel: 125s (+52%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can present the results boxplot and the p-values indicating the overlapping/disjoint probabilities over each concept score population with respect to its score population pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxplots(layer, metric):\n",
    "\n",
    "    def format_label_text(concept_name_list):\n",
    "\n",
    "        formated_list = list(concept_name_list)\n",
    "\n",
    "        for i, c in enumerate(concept_name_list):\n",
    "            if c.startswith('random'):\n",
    "                formated_list[i] = 'random'\n",
    "\n",
    "        return formated_list\n",
    "\n",
    "\n",
    "    n_plots = len(concept_list) + 1\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (25, 7 * 1))\n",
    "    fs = 18\n",
    "\n",
    "    for n in range(n_plots):\n",
    "\n",
    "        esl = experimental_set_list[n * n_exp : (n+1) * n_exp]\n",
    "        P1, P2, pval, relation = get_pval(scores, esl, layer, metric)\n",
    "\n",
    "        ax[n].set_ylim([0, 1])\n",
    "        ax[n].set_title(layer + \"-\" + metric + \" (pval=\" + str(pval) + \" - \" + relation + \")\", fontsize=fs)\n",
    "        ax[n].boxplot([P1, P2], showfliers=True)\n",
    "\n",
    "        ax[n].set_xticklabels(format_label_text(esl[0]), fontsize=fs)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxplots (\"inception4c\", \"magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxplots (\"inception4c\", \"sign_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxplots (\"inception4d\", \"magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxplots (\"inception4d\", \"sign_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores from every n_iter\n",
    "for layer in layer_list:\n",
    "    print(\"Layer:\", layer)\n",
    "    for i in range(len(concept_list) + 1):\n",
    "        print(\"Set:\", experimental_set_list[i * n_exp : (i+1) * n_exp])\n",
    "        print()\n",
    "        for metric in [\"sign_count\", \"magnitude\"]:\n",
    "            print(\"Metric:\", metric)\n",
    "            get_pval(scores, experimental_set_list[i * n_exp : (i+1) * n_exp], layer, metric, print_ret=True)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anp_cloned_from": {
   "notebook_id": "897261024067378",
   "revision_id": "1837855329685247"
  },
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "285697669316179"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200720-150132",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "deps": [
     "//pytorch/captum:bento_kernel_deps"
    ],
    "external_deps": []
   },
   "no_uii": true,
   "notebook_number": "310184",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "738845580229874",
   "tags": "",
   "tasks": "",
   "title": "TCAV - Tutorial I"
  },
  "kernelspec": {
   "display_name": "interpretability",
   "language": "python",
   "name": "bento_kernel_interpretability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
